import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader
from torch.autograd import Variable

from sklearn import preprocessing
import argparse
import numpy as np
import math
import os
import sys
import json
import io
import pickle as pkl
from tqdm import tqdm
import path_config as path
from bs4 import BeautifulSoup
import re
import difflib
import path_config as path
import utils
import random
from models import TextCNN,TextRCNN, TextRNN_Att
import pickle
import pprint

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

def strategy_A(word_list):
    # éšæœºä»å…¶ä»–ç±»é‡ç‚¹è¯ä¸­æ‰¾ä¸€ä¸ªä½œä¸ºæ›¿æ¢è¯
    tword = random.sample(word_list, 1)
    # print(tword)
    return next(iter(tword))

def strategy_B(embeddings,vocab, word, word_list):
    #ä»å…¶ä»–ç±»ä¸”æ‹¼å†™ç›¸ä¼¼é‡ç‚¹è¯ä¸­æ‰¾åˆ°ä¸€ä¸ªä¸ç›®æ ‡è¯ä½™å¼¦ç›¸ä¼¼åº¦æœ€å°çš„
    #éœ€è¦otherï¼Œè¦ä¹ˆå¤åˆ¶è¦ä¹ˆéƒ½æ”¾åœ¨replaceé‡Œï¼Œè¦ä¹ˆä»replaceé‡Œå¼•ç”¨
    # similar_word_list = difflib.get_close_matches(word, word_list, 5, cutoff=0.6)
    if word in vocab:
        wordid = utils.vocab[word]
    else:
        tword = find_least_leven(word, word_list,3)
        if tword:
            return next(iter(tword))
        else:
            return visual(word, similar_0)
    similar_word_list = find_least_leven(word, word_list,3)
    if len(similar_word_list):
        other_hotword_embeddings_sim = generate_embed(embeddings, similar_word_list, emb_dim=100)

        a = torch.FloatTensor(embeddings[(wordid)]).unsqueeze(dim=0)

        # b = torch.FloatTensor(np.delete(other_hotword_embeddings_sim, embeddings[(wordid)], axis=0))
        b = torch.FloatTensor(other_hotword_embeddings_sim)

        similarity = get_att_dis(target=a, behaviored=b)

        # similarity = -similarity

        sort_syn, ind = torch.sort(similarity, descending=True)

        tword = similar_word_list[ind[0]]
        # tword = list(utils.vocab.keys())[list(utils.vocab.values()).index(ind[0])]
        return tword
    else:
        tword = most_largest_cos_other(embeddings,vocab, word, word_list)
        return tword

def strategy_C(word, similar, all=False):
    #è§†è§‰ç›¸ä¼¼æ›¿æ¢ï¼Œå°†wordä¸­çš„æŸä¸€ä¸ªå­—æ¯æ›¿æ¢æˆè§†è§‰ç›¸ä¼¼å­—æ¯ï¼Œè¿”å›æ›¿æ¢åçš„è¯
    #éœ€è¦æ›¿æ¢è¡¨

    # for c in word:
    #     if c in similar:
    #         word = re.sub(c, similar[c], word)
    # return word

    if all:
        for c in word:
            if c in similar:
                visual_letters = similar[c]
                n = np.random.randint(0, len(visual_letters))
                word = re.sub(c, visual_letters[n], word)
        return word
    else:
        # print(len(word))
        s = np.random.randint(0, len(word))
        if word[s] in similar:
            visual_letters = similar[word[s]]
            n = np.random.randint(0, len(visual_letters))
            rletter = visual_letters[n]
        else:
            rletter = word[s]
        tword = word[:s] + rletter + word[s + 1:]
        return tword

def reserve_order(word):

    tword = u'\u202D' + u'\u2066'+ u'\u202E' + word[::-1] + u'\u202C' + u'\u2069' + u'\u202C'

    return tword

def repeat_insert(embeddings,vocab, word, word_list):
    n = np.random.randint(0, 2)

    cos_word = most_largest_cos_other_sim(embeddings, vocab, word, word_list)

    insert_word = u'\u0008'.join(s for s in cos_word) + u'\u0008'
    if n == 0:
        twords = word + ' '+ u'\u0008' + insert_word
    else:
        twords = insert_word + ' '+ u'\u0008' + word
    return twords

def A_repeat_insert(embeddings,vocab, word, word_list):
    n = np.random.randint(0, 2)
    tword = strategy_A(word_list)
    cos_word = most_largest_cos_other_sim(embeddings, vocab, word, word_list)

    insert_word = u'\u0008'.join(s for s in cos_word) + u'\u0008'
    if n == 0:
        twords = tword + ' '+ u'\u0008' + insert_word
    else:
        twords = insert_word + ' '+ u'\u0008' + tword
    return twords

def B_repeat_insert(embeddings,vocab, word, word_list):
    n = np.random.randint(0, 2)
    tword = strategy_B(embeddings,vocab, word, word_list)
    cos_word = most_largest_cos_other_sim(embeddings, vocab, word, word_list)

    insert_word = u'\u0008'.join(s for s in cos_word) + u'\u0008'
    if n == 0:
        twords = tword + ' '+ u'\u0008' + insert_word
    else:
        twords = insert_word + ' '+ u'\u0008' + tword
    return twords

def C_repeat_insert(embeddings,vocab, word, word_list):
    n = np.random.randint(0, 2)
    tword = strategy_C(word, similar_0, all=False)
    cos_word = most_largest_cos_other_sim(embeddings, vocab, word, word_list)

    insert_word = u'\u0008'.join(s for s in cos_word) + u'\u0008'
    if n == 0:
        twords = tword + ' '+ u'\u0008' + insert_word
    else:
        twords = insert_word + ' '+ u'\u0008' + tword
    return twords

def A_reserve_order(word_list):
    Aword = strategy_A(word_list)
    tword = reserve_order(Aword)
    return tword

def B_reserve_order(embeddings,vocab, word, word_list):
    Bword = strategy_B(embeddings,vocab, word, word_list)
    tword = reserve_order(Bword)
    return tword

def C_reserve_order(word):
    Cword = strategy_C(word, similar_0, all=False)
    tword = reserve_order(Cword)
    return tword

def remove_char(word):
    s = np.random.randint(0,len(word))
    if len(word)>1:
        tword = word[:s] + word[s+1:]
    else:
        tword = word

    return tword

def insert_char(word):

    s = np.random.randint(0,len(word)+1)
    tword = word[:s] + chr(97+np.random.randint(0,26)) + word[s:]

    return tword

def cut_apart(word):
    # char_list = ['Ì®', 'Ë—', 'Ë‘', ' Û£']
    if len(word)>1:
        s = np.random.randint(1,len(word))
        tword = word[:s] + 'Ì®' + word[s:]
    else:
        tword = word

    return tword

def transpose(word):
    splits = [(word[:i], word[i:]) for i in range(len(word) + 1)]
    tword = [a + b[1] + b[0] + b[2:] for a, b in splits if len(b)>1]
    return next(iter(tword))

def invisible_transpose(word):
    # tword = u'\u044f' + word #202E [::-1]
    # if len(word)>1:
    #     tword = word[:0] + u'\u202E' + word[0:]
    # else:
    #     tword = chr(97 + np.random.randint(0, 26))
    # return tword
    tword = u'\u202D' + u'\u2066' + u'\u202E' + word[::-1] + u'\u202C' + u'\u2069' + u'\u202C'
    # tword = u'\u202D' + u'\u202D' + u'\u202E' + word[::-1] + u'\u202C' + u'\u202C' + u'\u202C'
    # tword = u'\u202E' + word[::-1] + u'\u202C'
    return tword


def invisible_insert1(word): #ç›¸å½“äºinsert
    s = np.random.randint(0, len(word) + 1)
    tword = word[:s] + chr(97 + np.random.randint(0, 26)) + u'\u0008' + word[s:]
    return tword

def invisible_insert2(word):
    char_list = [u'\u200c', u'\u200d', u'\ufeff', u'\u2029']
    s = np.random.randint(0, len(word) + 1)
    tword = word[:s] + next(iter(random.sample(char_list, 1))) + word[s:]
    return tword

def visual(word, similar, all=False):
    #è§†è§‰ç›¸ä¼¼æ›¿æ¢ï¼Œå°†wordä¸­çš„æŸä¸€ä¸ªå­—æ¯æ›¿æ¢æˆè§†è§‰ç›¸ä¼¼å­—æ¯ï¼Œè¿”å›æ›¿æ¢åçš„è¯

    # for c in word:
    #     if c in similar:
    #         word = re.sub(c, similar[c], word)
    # return word

    if all:
        for c in word:
            if c in similar:
                visual_letters = similar[c]
                n = np.random.randint(0, len(visual_letters))
                word = re.sub(c, visual_letters[n], word)
        return word
    else:
        # print(len(word))
        s = np.random.randint(0, len(word))
        if word[s] in similar:
            visual_letters = similar[word[s]]
            n = np.random.randint(0, len(visual_letters))
            rletter = visual_letters[n]
        else:
            rletter = word[s]
        tword = word[:s] + rletter + word[s + 1:]
        return tword


def visual_replace1(embeddings,vocab, word, word_list):
    n = np.random.randint(0, 2)
    tword = visual(word, similar_0)
    cos_word = most_largest_cos_other_sim(embeddings, vocab, word, word_list)
    insert_word = u'\u0008'.join(s for s in cos_word) + u'\u0008'
    if n == 0:
        twords = tword + ' '+ u'\u0008' + insert_word
    else:
        twords = insert_word + ' '+ u'\u0008' + tword
    return twords

#è§†è§‰ç›¸ä¼¼å­—ç¬¦å¯¹ç…§è¡¨
similar_0 = {'-': 'ï¼', '9': 'ğŸ«', '8': 'ğŸª', '7': 'ğŸ©', '6': 'ï¼–', '5': 'ğŸ§', '4': 'ğŸ¦', '3': 'Ğ—', '2': 'ğŸ¸', '1': 'ğŸ£',
               '0': 'ğŸ¢', "'": 'Í´',".":"ï¼", 'a': 'Ğ°', 'b': 'ğ–»', 'c': 'Ï²', 'd': 'Ô', 'e': 'Ğµ', 'f': 'ğ–¿', 'g': 'ğš', 'h': 'Ò»',
               'i': 'Ñ–', 'j': 'Ï³', 'k': 'ğ—„', 'l': 'Î™', 'm': 'ï½', 'n': 'ğš—', 'o': 'Ğ¾', 'p': 'Ñ€', 'q': 'Ô›', 'r': 'ğš›',
               's': 'Ñ•', 't': 'ï½”', 'u': 'Ï…', 'v': 'Î½', 'w': 'Ô', 'x': 'Ñ…', 'y': 'Ñƒ', 'z': 'ğ—“', 'A': 'Ğ', 'B': 'Ğ’', 'C': 'Ğ¡',
                'D': 'ï¼¤', 'E': 'Î•', 'F': 'Ïœ', 'G': 'ÔŒ', 'H': 'Ğ', 'I': 'Ğ†', 'J': 'Ğˆ', 'K': 'Ğš', 'L': 'ğ–«', 'M': 'Ğœ', 'N': 'ğ–­', 'O': 'ÎŸ', 'P': 'Ğ ',
                'Q': 'Ôš', 'R': 'ğ–±', 'S': 'Ğ…', 'T': 'Ğ¢', 'U': 'ï¼µ', 'V': 'Ñ´', 'W': 'Ôœ', 'X': 'Ğ¥', 'Y': 'Ò®', 'Z': 'Î–',
           }

similar_advanced = {'-': 'ï¼Òƒ Ò„Ò‡', '9': 'ï¼™ğŸ«ğŸ¿Ï¤', '8': 'ï¼˜ğŸªğŸ¾', '7': 'ï¼—ğŸ©ğŸ½', '6': 'ï¼–ğŸ¨ğŸ¼Ğ±Ï¬', '5': 'ï¼•ğŸ§ğŸ»Ò”', '4': 'ï¼”ğŸ¦ğŸºĞ§', '3': 'Ğ—Ó ğŸ¥ğŸ¹ï¼“â†‹', '2': 'ï¼’ğŸ¤ğŸ¸Ï¨', '1': 'ï¼‘ğŸ£ğŸ·Ó',
               '0': 'ï¼ğŸ¢ğŸ¶Ğ', "'": ' Ò† ï¼‡Í´Î„',".":"ï¼Íµ", 'a': 'Ğ°ï½ğ–ºğšŠÎ±Î¬Ó‘Ó“', 'b': 'ï½‚ğ–»ğš‹Ğ¬Ï¦', 'c': 'ÑÏ²ï½ƒğ–¼ğšŒâ…½', 'd': 'Ôğ–½ğšï½„Ô€â…¾', 'e': 'Ğµğ–¾ğš ÑÑ‘Ò½Ó—ï½…Ï±', 'f': 'ï½†ğ–¿ğšÏ', 'g': 'ï½‡ğ—€ğš', 'h': 'Ò»Òºï½ˆğ—ğš‘Ô§Ô¦Ñ›',
               'i': 'Ñ–ï½‰â…°ğ—‚ğš’', 'j': 'Ñ˜Ï³ï½Šğ—ƒğš“', 'k': 'ğ—„ğš”ĞºÒŸÒ›ï½‹', 'l': 'Î™ğ—…ğš•ï½Œ', 'm': 'ï½â…¿ğ—†ğš–Ğ¼', 'n': 'ï½ğ—‡ğš—Î·Ğ¿', 'o': 'Î¿Ğ¾ğ—ˆğš˜Ó§ï½Ó©', 'p': 'Ñ€ï½ğ—‰ğš™ÏÒ', 'q': 'Ô›ğ—Šğššï½‘', 'r': 'ï½’ğ—‹ğš›',
               's': 'Ñ•ï½“ğ—Œğšœ', 't': 'ï½”Ï¯Ï„ğ—ğš', 'u': 'ï½•ğ—ğšÏ…Î¼', 'v': 'Î½ğ—ğšŸÑµï½–â…´', 'w': 'Ôğ—ğš Ñ¡ï½—Ï‰', 'x': 'Ñ…ğ—‘ğš¡Ó½Ó¿ï½˜â…¹', 'y': 'Ñƒğ—’ğš¢Ğ£ĞÓ¯ï½™Î³', 'z': 'ï½šğ—“ğš£â†Š', 'A': 'Ğï¼¡Î‘ğ– ğ™°Î›ÓÓ’', 'B': 'Ğ’Î’ğ–¡ğ™±ÏÎ²ï¼¢', 'C': 'Ğ¡Ï¹ğ–¢ğ™²â…­Ï¾ï¼£',
                'D': 'ï¼¤â…®ğ–£ğ™³', 'E': 'Î•ğ–¤ğ™´Ğ€ï¼¥ĞÓ–Ô', 'F': 'ğ–¥ğ™µÏœÒ’ï¼¦', 'G': 'ÔŒğ–¦ğ™¶ï¼§', 'H': 'Ğï¼¨ğ–§ğ™·Ó‰Ò¢', 'I': 'Ğ†Ó€Óğ–¨ğ™¸â… ï¼©', 'J': 'Í¿Ğˆğ–©ğ™¹ï¼ª', 'K': 'Ğšï¼«Îšğ–ªğ™ºÏÒšÒœÒĞŒ', 'L': 'ï¼¬ğ–«ğ™»â…¬', 'M': 'Ğœï¼­ÎœÏºğ–¬ğ™¼â…¯Ó', 'N': 'ï¼®ğ–­ğ™½Ğ˜', 'O': 'ÎŸĞğ–®ğ™¾ï¼¯Ó¦Ó¨', 'P': 'Ğ ï¼°Î¡ğ–¯ğ™¿Ï·Ò',
                'Q': 'Ôšï¼±ğ–°ğš€Ï˜', 'R': 'ï¼²ğ–±ğšĞ¯', 'S': 'Ğ…ï¼³ğ–²ğš‚Ï¨', 'T': 'Ğ¢ï¼´ğ–³ğšƒÎ¤Í²', 'U': 'ï¼µğ–´ğš„Ğ¦Ğ', 'V': 'Ñ´ğ–µğš…ï¼¶â…¤', 'W': 'Ôœğ–¶ğš†ï¼·', 'X': 'Ğ¥Î§ğ–·ğš‡â…©Ï‡Ò²Ó¼Ó¾ï¼¸', 'Y': 'Ò®ğ–¸ğšˆÎ¥Ï’ï¼¹', 'Z': 'Î–ï¼ºğ–¹ğš‰',
           }
# similar_0 = {'-': 'Ë—', '9': 'ê®', '8': 'ğŒš', '7': 'ğŸ•', '6': 'Ğ±', '5': 'Æ¼', '4': 'Ğ§', '3': 'Æ·', '2': 'Æ»', '1': 'l',
#                '0': 'O', "'": '`',".":"ï¼", 'a': 'É‘', 'b': 'Ğ¬', 'c': 'Ï²', 'd': 'Ô', 'e': 'Ğµ', 'f': 'ğš', 'g': 'É¡', 'h': 'Õ°',
#                'i': 'Ñ–', 'j': 'Ï³', 'k': 'â²•', 'l': 'â…¼', 'm': 'ï½', 'n': 'Õ¸', 'o': 'Ğ¾', 'p': 'Ñ€', 'q': 'Ô›', 'r': 'â²…',
#                's': 'Ñ•', 't': 'ğš', 'u': 'Õ½', 'v': 'Ñµ', 'w': 'Ô', 'x': 'Ã—', 'y': 'Ñƒ', 'z': 'á´¢', 'A': 'ğŠ ', 'B': 'ê“', 'C': 'Ï¹',
#                 'D': 'ê““', 'E': 'ğ‘€š', 'F': 'ğŠ‡', 'G': 'ÔŒ', 'H': 'â²', 'I': 'ğ‘€¡', 'J': 'á«', 'K': 'Æ˜', 'L': 'ğ‘€‰', 'M': 'Ïº', 'N': 'â²š', 'O': 'ğŒ', 'P': 'ğŠ•',
#                 'Q': 'â„š', 'R': 'Æ¦', 'S': 'Å', 'T': 'Æ¬', 'U': 'ğ“', 'V': 'Ñ´', 'W': 'ê“ª', 'X': 'â˜“', 'Y': 'Æ³', 'Z': 'áƒ',
#            }
# similar_1 = {'-': 'â€”', '9': 'à§­', '8': 'È¢', '7': 'ğ’‡', '6': 'â³’', '5': 'â´', '4': 'ğ‹Œ', '3': 'ê«', '2': 'á’¿', '1': 'ğ‘‘',
#                '0': 'á‹', "'": 'Ê¾',".":"ï¹’", 'a': 'â±­', 'b': 'á–¯', 'c': 'ğ½', 'd': 'â…¾', 'e': 'á¥±', 'f': 'ê¬µ', 'g': 'â„Š', 'h': 'â„',
#                'i': 'á¥', 'j': 'á’', 'k': 'Îº', 'l': 'á¥£', 'm': 'â…¿', 'n': 'â´–', 'o': 'à§¦', 'p': 'â²£', 'q': 'Õ¦', 'r': 'ê­‡',
#                's': 'êœ±', 't': 'Êˆ', 'u': 'á¥™', 'v': 'â…´', 'w': 'ê®ƒ', 'x': 'â²­', 'y': 'ê®', 'z': 'â²', 'A': 'ê“®', 'B': 'â„¬', 'C': 'â…­',
#                 'D': 'á ', 'E': 'â´¹', 'F': 'ê“', 'G': 'á€', 'H': 'â„‹', 'I': 'á†', 'J': 'á«', 'K': 'á¦', 'L': 'â„’', 'M': 'â„³', 'N': 'ê', 'O': 'Õ•', 'P': 'ê“‘',
#                 'Q': 'âµ•', 'R': 'â„›', 'S': 'ê•¶', 'T': 'ê”‹', 'U': 'ğ“', 'V': 'â…¤', 'W': 'ê“ª', 'X': 'â…©', 'Y': 'Æ³', 'Z': 'áƒ',
#            }
# similar_2 = {'-': 'â•´', '9': 'ğ’˜', '8': 'ğŒš', '7': 'ğ¨¬', '6': 'â³“', '5': 'Ò•', '4': 'êœ¬', '3': 'à«©', '2': 'Õ', '1': 'ß—',
#                '0': 'ğ‘ƒ°', "'": 'á¿½',".":"ï¼", 'a': 'ğ“Ÿ', 'b': 'Æ…', 'c': 'á¥´', 'd': 'ê“’', 'e': 'á§‰', 'f': 'Æ’', 'g': 'Ö', 'h': 'Òº',
#                'i': 'á¼°', 'j': 'Ú¶', 'k': 'â²•', 'l': 'ã€¡', 'm': 'â´…', 'n': 'ê‘', 'o': 'áƒ¿', 'p': 'ê‘', 'q': 'á¶', 'r': 'ê­ˆ',
#                's': 'àº£', 't': 'à©®', 'u': 'Å³', 'v': 'Î½', 'w': 'â±³', 'x': 'â…¹', 'y': 'Æ´', 'z': 'â´­', 'A': 'áª', 'B': 'ê•—', 'C': 'â„‚',
#                 'D': 'ğ‘€¥', 'E': 'ê“°', 'F': 'ğŠ‡', 'G': 'ê“–', 'H': 'á»', 'I': 'âµŠ', 'J': 'â°¬', 'K': 'â²”', 'L': 'ğ‘€‰', 'M': 'â…¯', 'N': 'â²š', 'O': 'âµ”', 'P': 'Ğ ',
#                 'Q': 'â„š', 'R': 'ğŠ¯', 'S': 'ğ ', 'T': 'ğŠ—', 'U': 'ğ¤‹', 'V': 'ğ°Ÿ', 'W': 'â±²', 'X': 'Î§', 'Y': 'ğŠ²', 'Z': 'â²Œ',
#            }
# similar_advanced = {'-': 'Ë—â€”Ë‰à±¼â•´â•¶â•¸â•ºâ•¼â•¾â•Œâ•âˆ½âˆ¼', '9': 'ê®ğ’˜ğ’›ê¯à­¨â’ê§¶ê”‡ğ¨³á§â³Šâ³‹ğ–áƒá‘«ğ‹á‹–á‚–Û¹à«§à§­à¦€ğ‘ƒ“Ç«',
#              '8': 'à§ªğŒšê–‰ğ È¢È£à©ªâ’ğ¤±Õ‘á‚˜á‚™ğŒ‡ğ©³ğŠ§ğê¸à­«', '7': 'ğŸ•â’ğ’‡ğ¨¬ğ“’ğ“ºá’£ğ©´âŒ‰Ë¥â…‚â”‘ãƒ¿ğ‘ê“¶â„¸ğ¨ª',
#              '6': 'á®á§ˆÏ­ğâ’ê®¾ğ‘†ê•ƒê§µê•„â³’â³“ğ‹‰á²œà¶»Õ³Ğ±áƒœÏ¬á‘²Ğ‘Æ‚Æƒ', '5': 'Æ½Æ¼â’Œâ´à½Ò”Ò•ã‚ŸğŸœªÑ’', '4': 'áâ’‹ê®êœ¬êœ­ğ‹ŒêœêğŸœ©êœ®êœ¯ã„ğ¨°ğ°ºÏ†â³ â³¡ğ¨’Ğ§à±ºáˆƒá”¦',
#              '3': 'ê«à±©áƒ•á²•â³Œâ³â„¨â³…êªê«á²™áƒ™à«©á´£ÈÈœá¦¡Æ·Ğ—Ğ·ÉœÓ â³„á²³á²Ó¡ğ¦¯ğ¨¢áƒâ’Šğ­£ğ­½ê’±ğ¤´à¥©Ò˜Ê“à§¬', '2': 'á’¿Æ¨Æ§â’‰Æ»á±®à§¨Õá˜–ğ­¥Ô¶â²¸â²¹àº‚ğ¨±á§“Õ·ß¶ğ‘ƒ¦à±½à³­à¥¨à¬Œ',
#              '1': 'ß—â’ˆğ°¯ğ‘‘â…¼à§·lâˆ£ã€¡â¼Ó€ê˜¡ï¸³','0': 'oOá‹Oê¬½ğ‘ƒ°ß€á±ğŒğ’†ê§°à¬ á±›ğ„â¬¯ğƒá‚ğ“‚á²¿ğ‘€â²á¦à­¦à§¦o', "'": "``â€˜Ê¹Ê¼Ê¾Ëˆá¿½à»ˆ",".":"ï¼ï¹’ï¼Â·Ë‘Ì©Ì£ Ü‚Ü",
#              'a': 'É‘â±­É‘ğ“Ÿğ’·ê­¤ğ“ ğ’¸ğ”ê¬°êœ¹â²€â²á¥²á¶Æ‹ÆŒá²›ğ€á¾³â±¥Ã£ÄÃ¡ÇÃ á¼€á¼á½°á½±á¾°á¾±á¾´á¾¶á¾·á¼‚á¼ƒá¼„á¼…á¼†á¼‡', 'b': 'á–¯ß•Æ…ğŒœáê®Ÿâ°“á²®á± â™­ğ‘ƒ¹Æ„á‘²á¶€ê—â¢áµ¬Æ€á–²Î²ÑŠÑŒĞªĞ¬',
#              'c': 'Ê—â²¤ê“šâ²¥ğ½ê®¯â…½á¥´Ï²áŸğ•â„‚âˆê®³á‘•Ï‚â†…á‚ ÆˆÆ‡Ò«Òªï¿ Ğ„Ğ¡Ì', 'd': 'Ôâ…¾ê“’ê’¯â…†ğ‘€˜á§ê®·á²«É—áµ­Ä‘á¶á–±á•·êÈ¡É–', 'e': 'Ğµá¥±á§‰á”â…‡ê¬²á¶’â±¸Ò¼Ò½á§™Ò¾Ò¿ê¬´â´’É‡Ãª',
#              'f': 'ğšê¬µÆ’â¨Ê„ê˜â¨ê­ê¬â¨ê™â¨‘á¶‚áµ³áµ®ß“Ò’Ò“Å¿á¶‹', 'g': 'É¡ê¬ê¬¶ê¡Ç¥â„Šê§¶ğ¤©É á¶ƒÄ¡Çµâ¡á§', 'h': 'Õ°Òºá‚á‚¹ê®’â„ğ±…êš”êš•â±¨ê•â„É¦á¥ê®µáŠ¨Ñ›áŠªáŠ­ã‚“É§Ñ’',
#              'i': 'Ñ–á¥ê­µâ…ˆÎ¯á½¶á½·á¼°á¼±ï­Ä¯á¶–É¨á‚ï­‘Ä«á¿‘Ã­Çá¿Ã¬â²“â„¹â²’', 'j': 'Ï³Ñ˜Ï³Ú¶á’â…‰ÊâŒ¡ê­»ÕµÈ·É‰á‚àº½ÄµÇ°â°¬â¼…ï»', 'k': 'ğ’ŒÎºê®¶â²•ğŠ‹â²”ê“—á›•á¦ĞšĞºÒšÒ›â±ªâ³¤Æ™Æ˜Ò Òê€ê‚ÒŸêêƒÒœê¢ê£ĞŒÑœ',
#              'l': '1à§·ê“²á¥£â…¼â„“×†âˆ£ã‡‘ê˜¡ã€¡â™â¼ï¸³â˜â²“É­á¶…È´Î¹Ó€Å‚ğ°¾', 'm': 'ï½â…¿á¶†áµ¯É±â´…ê¬ºá—°á²á¨“á±¬á›–ê“Ÿâ²˜â²™â±®â°®à´¨á™áŒ áŒ£áŒ¢áƒà·†àº•', 'n': 'Õ¸ê‘â´–á¶‡ê¥áµ°á±´Ö€Õ¼Õ²È Î·â´„ÈµĞ¿Å„ÅˆÇ¹á¼ á¼¡á¼¢á¼£á¼¤á¼¥á¼¦á¼§Ğ»',
#              'o': 'Ğ¾à§¦0Ö…à­¦á¦á§à±¦à»à¹ğ“ªáƒ¿ê“³ğ„â²Ÿâ—‹ğŒğ“‚â±ºğƒê¬½áŸ á²¿ğ‘€á±›ß‹Æ¡â—¯ooĞÏƒÅÃ³Ç’Ã²', 'p': 'Ñ€ê“‘ê®²á¢â²¢â²£á´˜á´©ğŒ“ğŒ›ğŠ•á±ğ“„Ç·â°ê’ê“áµ±êê‘â„™áµ½ê¥á¶ˆá‘­ğ™â´áš¹Ïá‹¨Ö„á–°Æ¥', 'q': 'Ô›á‚·Õ¦Ô›Ô›É‹á¶ê—á‘«àªµà«§Û¹ÉŠê™á–³á›©á•´à¦€á‚–Ê à§­Ç«Æ',
#              'r': 'ê­‡ê­ˆê„É¼ğ°®ğŠ„ğŠê»ê¼ê§êµáµ²Ê³Éá¶‰É½ê”µáš´É¤Å•Å™Ğ³Ğ“ê©','s': 'Ñ•êœ±ê®ªğ¨¡á²½ğ ğ’–êš‚êšƒê“¢áƒ½Õà´Ÿášàº£á‚½à²½àª½à¤½ğŠ–Ù‰â‘€á¶Šáµ´ÛÛŒá¦$Ê‚â±¾', 't': 'ğšÆ­Êˆà©®áµ¼ê®â±¦áµµÏ„È¶Å£Å§Æ«',
#              'u': 'Õ½Õ½á¥™ê­’Æ°ÂµÅ³â´ê­â´—á¶™áˆ€Ñ‡Ñ†Å«ÃºÇ”Ã¹Ğ¦', 'v': 'Ñµá´ âˆ¨ê®©á™â…´â…¤ğ°Ÿâ´¸â‹Î½â±±ê®´Ñ´êêŸâ°œâ±´á¶Œ', 'w': 'Ôê®ƒê“ªá´¡á³â±³â±²êŸ½ğá”ê®¤à¸à¸œà¸Ÿà¸ê ê¡á—¯á™Õ¡â´É¯ÆœÏ‰ÑˆÑ‰Ğ¨Ğ©',
#              'x': 'Ã—ã„¨ã€¤ê“«áµ¡â²­â•³â…©âµâ²¬â…¹Ï‡Ã—âœ•Ò²ê’¼Ò³âœ—âœ˜â¤«â¤¬á¶ê³âœ–ê­“ê­”ê­•ê­–ê­—ê­˜ê­™ğŒ¢ğŒ—ğ°“ğ‘€‹ğ‘€Œ', 'y': 'Ñƒá©Î³Ğ£ê­¹á½ê®Æ´â´˜á‚¸â´á²§áƒ§ê­šêšÒ¯Ï’ĞÑÓ²Ó³É¥â„½Æ”ê¨ê©', 'z': 'á´¢á´¢â²ê®“â´­áƒê“œáƒâ²Œâ„¤â±¬ğ“»ğ““È¤â±«â±¿È¥á‚—É€Êá±®áµ¶Õ€Æ¶Å¼Ê‘á¶Î¶',
#              'A': 'ê“®Î‘ğŠ ê­ºĞáªÄ„á¯ğŒ€ê­¿ê¬…ğ’²ğ‹Î›Ñ¦Èºâ‚³È¦Î†Ã€ÃÃ‚ÃƒÃ„Ã…Ä€Ä‚áº¢áº á¸€ÓÓ’á¾¸á¾¹â’¶ï¿½', 'B': 'Î’ê•—á´â²‚ê“ğŠ‚Ğ’ğŒğŠ¡â„¬ğ°ê–ÆÉƒê´ÃŸÏá°ğšêµáºá›’á›”á¸‚á¸„á¸†â‚¿â’·', 'C': 'Ï¹ê“šĞ¡ğ‘€áŸğ•Ê—â²¤ğŒ‚â…­ê’ê’â„‚Æ‡ğ’¿ğ“†á£Òªâ†…Ã‡ğ¤Œáˆ­â‚¡È»â‚µÄ†ÄˆÄŠÄŒá¢·á¸ˆâ‚¢â’¸',
#              'D': 'á ê­°ê““ÆŠğ“ˆğ‘€¥ğ“‰â……â†â…®ğ¤†ğŸğ°–ğŒƒÄÃÆ‰Äá¸Šá¸’á¸á¸Œá¸â’¹', 'E': 'Î•ê“°ğŒ„ğŠ¤â´¹á¬ğ‘€šê­¼ğŠ†Ä˜È¨áº¸á¸šÉ†áººÎˆÄ–ÃˆĞ€Ã‰ÃŠÃ‹Ä’áº¼Ä”Äšá¸˜á¸”á¸–á¸œĞĞ„Ó–â’º', 'F': 'ê“ğ °ğŒ…ğŠ‡ğŠ¥Æ‘ê˜â„±Óºáš©ášªáš«Ïœáš¨á¸Î“â‚£â’»',
#              'G': 'ÔŒê“–á€á©áµá¶Æ“á³Ç¤á¸ ê â‚²Ä¢Ä Ç´ÄœÄÇ¦â’¼ê®†ê®ê®¹', 'H': 'Ğê“§â„‹â²êªê–¾â„á»ê€¿êƒ…ğ‹Î—Ò¢ğ‘€…Ó‰â±§Ò¤ášºá‹˜á‹šá¸¨á¸¢á¸¤á¸ªÓ‡êœ¦ğ’€‚á‹à¨®àª®á‹™ÔŠÇ¶Î‰Ä¤Ä¦á‹œá¸¦â’½', 'I': 'á†ğŒ†Î™ê“²ğ°¾ğ‘‡âµâŒ¶ğ‘€¡ğŠˆâµŠâ… ê€¤Ğ†Óá›Ä®á»ˆá»ŠÄ°á¸¬ÃŒÃÃÃÄ¨ÄªÄ¬ÎŠÎªá¸®Ğ‡á¿˜á¿™â’¾',
#              'J': 'á«ê“™Ğˆê­»ê²à¨¹ğ‘€®Ä´àº½â°¬â’¿', 'K': 'Îšê“—â„ªá¦á›•â²”ê—£ÏÒšâ±©ÒœÒê€ê‚ê„Ò â‚­ê¢Ä¶á¸²Æ˜Ğšá¸´ĞŒá¸°Ç¨Óƒğ’¼â“€', 'L': 'áâ…¬ğ‘€‰ğ›ğŒ‹ê“¡â„’ê’’ê†á¸¶âŒŠê­á¸ºá¸¼ÅÈ½â± â±¢Ä¿Ä¹Ä»Ä½á¢ºá¸¸â“ê®®', 'M': 'Ïºê“ŸĞœâ…¯ğŒ‘â²˜Îœá·â±®á›–ğ°¡â„³ğŠ°ê™¦Óá¹‚á›—á¸¾á¹€â“‚ê®‡',
#              'N': 'Îê“ â²šêğŠªÅŠÆâ„•Í¶Ğ˜Å…Ã‘ÅƒÅ‡Ç¸â‚¦ê¤â“ƒ', 'O': '0Õ•ğŒê“³â²âµ”ğ„ğ“‚á²¿Ğá±›á‹ÎŸê™¨Ê˜Î˜á¾â²Ó¨Ñ²á«Æ á»ŒÃ’Ã“Ã”Ã•Ã–ÅŒÅÅÎŒá¤á¹Œá¹á¹á¹’Ó¦â“„', 'P': 'Î¡ê“‘Ğ ğŒ“ğŠ•â„™â²¢êê’Æ¤×§á¢ğ°™áš¹â±£á¹”á¹–Ç·Òâ‚±â‚½â“…',
#              'Q': 'Ôšâµ•â„šê–ê˜ğŒ’ÇªÇ¬Ï˜â“†', 'R': 'á’Æ¦ğ’´ğŠ¯ê­±ê“£ê­†á¡áš±ÉŒâ„›â„œâ„â±¤â„ê¦â„Ÿá¹šÅ–á¹Å”Å˜á¹˜á¹œâ“‡ê®¢', 'S': 'Ğ…ê•¶ğ êš‚ê“¢ášê—Ÿê’šá²½á¹¨ê•·Åâ±¾ê¨á¦á¹¢á¹ ÅšÅœÅ á¹¤á¹¦È˜â‚·â“ˆ',
#              'T': 'Î¤ğŒ•ê”‹ğŠ—ê­²ê“”ğŠ±â²¦Ğ¢Í²ê“„Ò¬á¢Í³Æ¬Æ®È¾á¹¬Å¢Å¦á¹®á¹°á¹ªÅ¤Èšâ‚®â“‰', 'U': 'ğ“ğ¤‹ê“´Æ¯Æ²áˆ…Å²É„á»¤á¹²á¹´á¹¶á»¦Ã™ÃšÃ›ÃœÅ¨á¹¸á¹ºá»°á»¨á»ªá»¬È–â“Š', 'V': 'á™ğ°Ÿâ…¤á¤ğ ¨Ñ´á¹¾á¹¼Ñ¶êâ„£â“‹ê®©',
#              'W': 'ê“ªá³ê¶á”â±²ê áºˆáº†áº€áº‚Å´áº„ê®ƒâ‚©â“Œâ°', 'X': 'Î§â²¬ê³ğ°“ê“«â…©â˜“âœ•ğŠ´ê’¼ê­“ê­”ê­•Ï‡Ó¼Ó¾Ğ¥Ò²áš·á²¯áºŠáºŒâ“', 'Y': 'Î¥ê“¬â²¨ğŒ–Ï’Ò®ğ° Æ³ğŠ²ê¨á»´áºÒ°É×¥Ãá»²Å¶á»¶á»¸Å¸ÎÎ«Ï“Ï”Æ”È²á¿¨á¿©â“',
#              'Z': 'Î–áƒê“œâ²Œá‚—È¤ğ““â±«â„¤ê™€ê™‚Æµâ±¿áº’áº”Å»Å¹Å½áºâ“ê®“'}


default_filter = '!"#$%&()*+,-./:;<=>?@[\\]^_`{|}~\t\n'


default_alphabet = "abcdefghijklmnopqrstuvwxyz0123456789-,;.!?:\'\"/\\|_@#$%^&*~`+ =<>()[]{}"

def replace_insert(embeddings,vocab, word, word_list):
    n = np.random.randint(0, 2)
    tword = most_largest_cos_other(embeddings, vocab, word, word_list)
    insert_word = invisible_insert1(word)
    if n == 0:
        twords = tword + ' ' + insert_word
    else:
        twords = insert_word + ' ' + tword
    return twords

def replace_insert_sim(embeddings,vocab, word, word_list):
    n = np.random.randint(0, 2)
    tword = most_largest_cos_other_sim(embeddings, vocab, word, word_list)
    # insert_word = invisible_insert1(word)
    insert_word = tword
    # insert_word = next(iter(find_most_similiar(word, word_list, 0.2)))
    if n == 0:
        twords = tword + ' ' + insert_word
    else:
        twords = insert_word + ' ' + tword
    return twords

def invisible_insert1_replace1(embeddings,vocab, word, word_list):
    n = np.random.randint(0, 2)
    tword = invisible_insert2(word)
    cos_word = most_largest_cos_other_sim(embeddings, vocab, word, word_list)
    insert_word = u'\u0008'.join(s for s in cos_word) + u'\u0008'
    if n == 0:
        twords = tword + ' ' + u'\u0008' + insert_word
    else:
        twords = insert_word + ' ' + u'\u0008' + tword
    return twords

def invisible_transpose_replace1(embeddings,vocab, word, word_list):
    n = np.random.randint(0, 2)
    tword = invisible_transpose(word)
    cos_word = most_largest_cos_other_sim(embeddings, vocab, word, word_list)
    insert_word = u'\u0008'.join(s for s in cos_word) + u'\u0008'
    if n == 0:
        twords = tword + ' ' + u'\u0008' + insert_word
    else:
        twords = insert_word + ' ' + u'\u0008' + tword
    return twords

def encode(s):
    return ''.join([bin(ord(c)).replace('0b', '') for c in s])

def invisible_insert1_replace2(embeddings,vocab, word, word_list):
    n = np.random.randint(0, 2)
    tword = invisible_insert1(word)
    # cos_word = most_largest_cos_other_sim(embeddings, vocab, word, word_list)
    insert_word = u'\u0008' + encode(tword).replace('0', u'\u200d').replace('1', u'\u200c')
    if n == 0:
        twords = tword + ' ' + insert_word
    else:
        twords = insert_word + ' ' + tword
    return twords

def invisible_transpose_replace2(embeddings,vocab, word, word_list):
    n = np.random.randint(0, 2)
    tword = invisible_transpose(word)
    # cos_word = most_largest_cos_other_sim(embeddings, vocab, word, word_list)
    insert_word = u'\u0008' + encode(tword).replace('0', u'\u200d').replace('1', u'\u200c')
    if n == 0:
        twords = tword + ' ' + insert_word
    else:
        twords = insert_word + ' ' + tword
    return twords


def find_most_similiar(word, word_list, similarity=0.6):
    #æ‰¾åˆ°æ‹¼å†™ä¸Šç›¸ä¼¼çš„è¯ï¼Œæ˜¯å­—ç¬¦ä¸²æ‹¼å†™ä¸Šçš„ï¼Œeg.ç”±v100r001æ‰¾åˆ°v100r003
    tword = difflib.get_close_matches(word, word_list, 1, cutoff=similarity)

    return tword

import Levenshtein
# def find_least_leven(word, word_list):
#     #æ‰¾åˆ°ç¼–è¾‘è·ç¦»æœ€å°çš„è¯
#     # sub_list = difflib.get_close_matches(word, word_list, cutoff=similarity)
#     dis = [Levenshtein.ratio(word,word1) for word1 in word_list]
#     tword = word_list[dis.index(max(dis))]
#
#     return tword

def find_least_leven(word, word_list, number=1, distance=3):
    #æ‰¾åˆ°ç¼–è¾‘è·ç¦»æœ€å°çš„è¯
    # sub_list = difflib.get_close_matches(word, word_list, cutoff=similarity)
    dis = {}
    for word1 in word_list:
        if Levenshtein.distance(word, word1) <= distance:
            dis[word1] = Levenshtein.distance(word, word1)
    # dis = [Levenshtein.ratio(word,word1) for word1 in word_list]
    if len(dis) > 0:
        M = sorted(dis.items(), key=lambda x: x[1])[:number]
        # print(M)
        # tword = word_list[dis.index(max(dis))]
        if number == 1:
            for m in M:
                tword = m[0]
        else:
            tword = []
            for m in M:
                tword.append(m[0])
    else:
        tword = []

    return tword

def leven_replace1(embeddings,vocab, word, word_list):
    n = np.random.randint(0, 2)
    tword = find_least_leven(word, word_list)
    cos_word = most_largest_cos_other_sim(embeddings, vocab, word, word_list)
    insert_word = u'\u0008'.join(s for s in cos_word) + u'\u0008'
    if n == 0:
        twords = tword + ' ' + u'\u0008' + insert_word
    else:
        twords = insert_word + ' ' + u'\u0008' + tword
    return twords

def cossim_replace1(embeddings,vocab, word, word_list):
    n = np.random.randint(0, 2)
    tword = most_largest_cos_other_sim(embeddings, vocab, word, word_list)
    cos_word = most_largest_cos_other_sim(embeddings, vocab, word, word_list)
    # cos_word = invisible_insert1(word)
    insert_word = u'\u0008'.join(s for s in cos_word) + u'\u0008'
    if n == 0:
        twords = tword + ' ' + u'\u0008' + insert_word
    else:
        twords = insert_word + ' ' + u'\u0008' + tword
    return twords

def random_select(word_list):
    #éšæœºä»å…¶ä»–ç±»é‡ç‚¹è¯ä¸­æ‰¾ä¸€ä¸ªä½œä¸ºæ›¿æ¢è¯
    tword = random.sample(word_list, 1)

    return tword

#ä»¥ä¸‹ä¸‰ä¸ªå‡½æ•°æ˜¯æ±‚ä½™å¼¦ç›¸ä¼¼åº¦çš„
def get_att_dis(target, behaviored):

        attention_distribution = []

        for i in range(behaviored.size(0)):
            attention_score = torch.cosine_similarity(target, behaviored[i].view(1, -1))  # è®¡ç®—æ¯ä¸€ä¸ªå…ƒç´ ä¸ç»™å®šå…ƒç´ çš„ä½™å¼¦ç›¸ä¼¼åº¦
            attention_distribution.append(attention_score)
        attention_distribution = torch.Tensor(attention_distribution)

        return attention_distribution / torch.sum(attention_distribution, 0)  # æ ‡å‡†åŒ–

def find_syns(embeddings, wordid):


        a = torch.FloatTensor(embeddings[(wordid)]).unsqueeze(dim=0)
        # print(wordid)

        b = torch.FloatTensor(np.delete(embeddings, wordid, axis=0))


        similarity = get_att_dis(target=a, behaviored=b)
        return similarity

def most_largest_cos_all(embeddings, word):
    #ä»æ‰€æœ‰æ‰€æœ‰è¯ä¸­æ‰¾åˆ°ä¸€ä¸ªä¸ç›®æ ‡è¯ä½™å¼¦ç›¸ä¼¼åº¦æœ€å¤§çš„
    wordid = utils.vocab[word]
    sort_syn, ind = torch.sort(find_syns(embeddings, wordid), descending=True)

    # print(list(utils.vocab.keys())[list(utils.vocab.values()).index(wordid)])
    # return torch.FloatTensor(embeddings[(ind[0])]).unsqueeze(dim=0)
    # return ind[0]
    tword = list(utils.vocab.keys())[list(utils.vocab.values()).index(ind[0])]
    return tword


def generate_embed(embeddings, word_list, emb_dim=100):
    """æ ¹æ®å…¶ä»–ç±»é‡ç‚¹è¯è¯è¡¨æˆè¯å‘é‡çŸ©é˜µ"""
    other_hotword_embeddings = np.random.rand(len(word_list), emb_dim)

    for idx in range(len(word_list)):
        # print(embeddings[utils.vocab[word_list[idx]]])
        other_hotword_embeddings[idx, :] = embeddings[(utils.vocab[word_list[idx]])]  # è¯å‘é‡çŸ©é˜µ
    # print(other_hot_word_embeddings)
    # np.savez_compressed(path.embed_path, embeddings=embeddings)
    return other_hotword_embeddings

def most_largest_cos_other(embeddings,vocab, word, word_list):
    #ä»å…¶ä»–ç±»é‡ç‚¹è¯ä¸­æ‰¾åˆ°ä¸€ä¸ªä¸ç›®æ ‡è¯ä½™å¼¦ç›¸ä¼¼åº¦æœ€å¤§çš„
    if word in vocab:
        wordid = utils.vocab[word]
    else:
        tword = find_least_leven(word, word_list)
        if tword:
            return next(iter(tword))
        else:
            return visual(word, similar_0)
    other_hotword_embeddings = generate_embed(embeddings, word_list, emb_dim=100)

    a = torch.FloatTensor(embeddings[(wordid)]).unsqueeze(dim=0)

    # b = torch.FloatTensor(np.delete(other_hotword_embeddings, embeddings[(wordid)], axis=0))
    b = torch.FloatTensor(other_hotword_embeddings)
    # print(word)
    # print(wordid)

    similarity = get_att_dis(target=a, behaviored=b)

    similarity = -similarity

    sort_syn, ind = torch.sort(similarity)
    # print(similarity)
    # print(sort_syn)
    # print(ind)

    tword = word_list[ind[0]]
    # tword = list(utils.vocab.keys())[list(utils.vocab.values()).index(ind[0])]
    return tword

def most_largest_cos_other_sim(embeddings,vocab, word, word_list):
    #ä»å…¶ä»–ç±»ä¸”æ‹¼å†™ç›¸ä¼¼é‡ç‚¹è¯ä¸­æ‰¾åˆ°ä¸€ä¸ªä¸ç›®æ ‡è¯ä½™å¼¦ç›¸ä¼¼åº¦æœ€å¤§çš„
    # similar_word_list = difflib.get_close_matches(word, word_list, 5, cutoff=0.6)
    if word in vocab:
        wordid = utils.vocab[word]
    else:
        # tword = find_most_similiar(word, word_list)
        tword = find_least_leven(word, word_list)
        if tword:
            return next(iter(tword))
        else:
            return visual(word, similar_0)
    similar_word_list = find_least_leven(word, word_list,5)
    if len(similar_word_list):
        other_hotword_embeddings_sim = generate_embed(embeddings, similar_word_list, emb_dim=100)

        a = torch.FloatTensor(embeddings[(wordid)]).unsqueeze(dim=0)

        # b = torch.FloatTensor(np.delete(other_hotword_embeddings_sim, embeddings[(wordid)], axis=0))
        b = torch.FloatTensor(other_hotword_embeddings_sim)

        similarity = get_att_dis(target=a, behaviored=b)

        similarity = -similarity

        sort_syn, ind = torch.sort(similarity)

        tword = similar_word_list[ind[0]]
        # tword = list(utils.vocab.keys())[list(utils.vocab.values()).index(ind[0])]
        return tword
    else:
        tword = most_largest_cos_other(embeddings,vocab, word, word_list)
        return tword

#ä»å…¶ä»–ç±»å•è¯ä¸­æ‰¾å‡ºæ¢¯åº¦æ–¹å‘æŠ•å½±è·ç¦»æœ€å¤§çš„ï¼ˆä»…é™åŸºäºæ¨¡å‹ç™½ç›’ï¼‰,textsæœªç»rawå¤„ç†
def most_largest_projection(word_index, model, texts, embeddings, vocab, word, word_list):

    if word in vocab:
        wordid = utils.vocab[word]
    else:
        tword = find_most_similiar(word, word_list)
        if tword:
            return next(iter(tword))
        else:
            return visual(word, similar_0)
    input_seq, f_label= utils.build_text(texts,vocab)

    # word_index = input_seq.index(utils.vocab[word])


    input_seq = torch.Tensor(input_seq).long().view(1, -1).unsqueeze(dim=0)
    if device:
        input_seq = input_seq.to(device)

    if isinstance(model, torch.nn.DataParallel):
        model = model.module

    embd, output = model(input_seq, returnembd=True)
    tlabel = torch.max(output, 1)[1].view(-1)

    embd.retain_grad()

    loss = F.nll_loss(output, torch.Tensor([float(tlabel)]).long().to(device))
    loss.backward()


    if len(word_list):

        syn = []
        for i in range(len(word_list)):

            y= embd.grad[0][word_index]

            x= embeddings[utils.vocab[word_list[i]]]

            # projection = y * np.dot(x, y) / np.dot(y, y)
            #
            # projection = torch.norm(projection, p=2).item()
            y_square = torch.norm(y, p=2).item()
            distance = (x - embeddings[wordid]) * y_square
            distance = torch.norm(torch.FloatTensor(distance), p=2).item()

            syn.append(distance)


        tword = word_list[syn.index(max(syn))]
        return tword
    else:
        tword = find_most_similiar(word, word_list)
        if tword:
            return next(iter(tword))
        else:
            return visual(word, similar_0)

def get_same_element_index(ob_list, word):

    return [i+1 for (i, v) in enumerate(ob_list) if v == word]
#ä»å…¶ä»–ç±»ã€æ‹¼å†™ç›¸ä¼¼çš„å•è¯ä¸­æ‰¾å‡ºæ¢¯åº¦æ–¹å‘æŠ•å½±è·ç¦»æœ€å¤§çš„ï¼ˆä»…é™åŸºäºæ¨¡å‹ç™½ç›’ï¼‰,textsæœªç»rawå¤„ç†
def most_largest_projection_sim(word_index, model, texts, embeddings, vocab, word, word_list):

    if word in vocab:
        wordid = utils.vocab[word]
    else:
        tword = find_most_similiar(word, word_list)
        if tword:
            return next(iter(tword))
        else:
            return visual(word, similar_0)
    input_seq, f_label= utils.build_text(texts,vocab)
    # word_index = input_seq.index(utils.vocab[word])
    # word_index_list = get_same_element_index(input_seq, utils.vocab[word])


    input_seq = torch.Tensor(input_seq).long().view(1, -1).unsqueeze(dim=0)
    if device:
        input_seq = input_seq.to(device)

    if isinstance(model, torch.nn.DataParallel):
        model = model.module
    # model.train()
    banner = input_seq.clone()
    # print(banner)
    embd, output = model(input_seq, returnembd=True)
    tlabel = torch.max(output, 1)[1].view(-1)
    # print(embeddings[utils.vocab[word]])
    embd.retain_grad()

    loss = F.nll_loss(output, torch.Tensor([float(tlabel)]).long().to(device))
    loss.backward()
    # print(embd.grad[0])
    # y= embd.grad[0][word_index]
    # x= embeddings[utils.vocab['c1000z']]
    # projection = y * np.dot(x, y) / np.dot(y, y)
    # projection = torch.norm(projection, p=2)
    similar_word_list = difflib.get_close_matches(word, word_list, 5, cutoff=0.6)
    if len(similar_word_list):
        # syn = np.random.rand(len(similar_word_list),1)
        syn = []

        for i in range(len(similar_word_list)):
            # print(i)
            y= embd.grad[0][word_index]

            x= embeddings[utils.vocab[similar_word_list[i]]]

            # projection = y * np.dot(x, y) / np.dot(y, y)
            #
            # projection = torch.norm(projection, p=2).item()

            # x_square = torch.norm(torch.FloatTensor(x), p=2).item()
            y_square = torch.norm(y, p=2).item()
            # distance = (x_square - projection)/x_square * y_square
            distance = (x - embeddings[wordid]) * y_square
            distance = torch.norm(torch.FloatTensor(distance), p=2).item()
            # print(distance)
            # syn[i, :] = projection
            syn.append(distance)

        # sort_syn, ind = torch.sort(torch.FloatTensor(syn))
        # print(syn)
        # print(similar_word_list[syn.index(max(syn))])
        tword = similar_word_list[syn.index(max(syn))]
        # print(word_index)
        # print(tword)
        return tword
    else:
        tword = most_largest_projection(word_index, model, texts, embeddings, vocab, word, word_list)
        return tword
#ç»“åˆä½™å¼¦å€¼å’ŒæŠ•å½±å‘é‡çš„é•¿åº¦ï¼šä½™å¼¦å€¼å’ŒæŠ•å½±å‘é‡çš„é•¿åº¦éƒ½è¦å°½é‡å°
#ä»å…¶ä»–ç±»è¯ä¸­é€‰
def cos_proj_combine(word_index, model, texts, embeddings, vocab, word, word_list):
    if word in vocab:
        wordid = utils.vocab[word]
    else:
        tword = find_most_similiar(word, word_list)
        if tword:
            return next(iter(tword))
        else:
            return visual(word, similar_0)

    input_seq, f_label = utils.build_text(texts, vocab)

    # word_index = input_seq.index(utils.vocab[word])

    input_seq = torch.Tensor(input_seq).long().view(1, -1).unsqueeze(dim=0)
    if device:
        input_seq = input_seq.to(device)

    if isinstance(model, torch.nn.DataParallel):
        model = model.module

    embd, output = model(input_seq, returnembd=True)
    tlabel = torch.max(output, 1)[1].view(-1)

    embd.retain_grad()

    loss = F.nll_loss(output, torch.Tensor([float(tlabel)]).long().to(device))
    loss.backward()


    if len(word_list):
        # syn = np.random.rand(len(similar_word_list),1)
        syn = []
        for i in range(len(word_list)):
            # print(i)
            y = embd.grad[0][word_index]

            x = embeddings[utils.vocab[word_list[i]]]
            x = x - embeddings[wordid]
            y = y.cpu()
            y_square = torch.norm(y, p=2).item()

            # projection = y * np.dot(x, y) / np.dot(y, y)
            #
            # projection = torch.norm(projection, p=2).item()
            # print(projection)
            a = np.dot(x.dot(y), y) / y_square
            projection = x - a
            # projection = torch.norm(projection, p=2).item()
            projection = np.linalg.norm(projection)

            cos = x.dot(y)/(np.linalg.norm(x)*np.linalg.norm(y))
            # print(type(cos))

            distance = projection * np.sign(cos)
            # print(cos)
            # distance = torch.norm(torch.FloatTensor(distance), p=2).item()

            syn.append(distance)


        tword = word_list[syn.index(min(syn))]
        return tword
    else:
        tword = find_most_similiar(word, word_list)
        if tword:
            return next(iter(tword))
        else:
            return visual(word, similar_0)


#ä»å…¶ä»–ç±»è¯ä¸­æ‹¼å†™ç›¸ä¼¼çš„é‡Œé¢é€‰
def cos_proj_combine_sim(word_index,model, texts, embeddings,vocab, word, word_list):
    # similar_word_list = difflib.get_close_matches(word, word_list, 5, cutoff=0.6)
    # print(similar_word_list)
    if word in vocab:
        wordid = utils.vocab[word]
    else:
        tword = find_most_similiar(word, word_list)
        if tword:
            return next(iter(tword))
        else:
            return visual(word, similar_0)

    similar_word_list = difflib.get_close_matches(word, word_list, 5, cutoff=0.6)
    input_seq, f_label = utils.build_text(texts, vocab)

    # word_index = input_seq.index(utils.vocab[word])

    input_seq = torch.Tensor(input_seq).long().view(1, -1).unsqueeze(dim=0)
    if device:
        input_seq = input_seq.to(device)

    if isinstance(model, torch.nn.DataParallel):
        model = model.module

    embd, output = model(input_seq, returnembd=True)
    tlabel = torch.max(output, 1)[1].view(-1)

    embd.retain_grad()

    loss = F.nll_loss(output, torch.Tensor([float(tlabel)]).long().to(device))
    loss.backward()


    if len(similar_word_list):
        # syn = np.random.rand(len(similar_word_list),1)
        syn = []
        for i in range(len(similar_word_list)):
            # print(i)
            y = embd.grad[0][word_index]

            x = embeddings[utils.vocab[similar_word_list[i]]]
            x = x - embeddings[wordid]
            y = y.cpu()
            y_square = torch.norm(y, p=2).item()


            a = np.dot(x.dot(y), y) / y_square
            projection = x - a
            # projection = torch.norm(projection, p=2).item()
            projection = np.linalg.norm(projection)

            # projection = y * np.dot(x, y) / np.dot(y, y)
            #
            # projection = torch.norm(projection, p=2).item()
            # print(projection)

            cos = x.dot(y)/(np.linalg.norm(x)*np.linalg.norm(y))
            # print(cos)

            distance = projection * np.sign(cos)
            # distance = torch.norm(torch.FloatTensor(distance), p=2).item()

            syn.append(distance)


        tword = similar_word_list[syn.index(min(syn))]
        return tword
    else:
        tword = cos_proj_combine(word_index,model, texts, embeddings,vocab, word, word_list)
        return tword



#ä»å…¶å®ƒç±»å•è¯ä¸­æ‰¾å‡ºå¯¹ç½®ä¿¡åº¦å½±å“æœ€å¤§çš„è¯,ï¼ˆé»‘ç›’å¯ä»¥ç”¨ï¼Ÿå­˜ç–‘ï¼‰,textsæœªç»rawå¤„ç†,å…¶ä»–ç±»çš„è¯è¡¨é•¿æ—¶ä¼šå¾ˆæ…¢
def most_largest_conf(word_index,model, texts,vocab, word, word_list):
    word_id_list = []
    for i in range(len(word_list)):
        word_id_list.append(utils.vocab[word_list[i]])

    input_seq, f_label = utils.build_text(texts,vocab)

    # word_index = input_seq.index(utils.vocab[word])

    input_seq = torch.Tensor(input_seq).long().view(1, -1).unsqueeze(dim=0)
    if device:
        input_seq = input_seq.to(device)
    res = model(input_seq)

    tlabel = torch.max(res, 1)[1].view(-1)

    if len(word_list):
        syn = []
        tempinputs = input_seq.clone()

        for word_id in word_id_list:

            tempinputs[0] = torch.LongTensor([word_id if i==word_index else a for a in input_seq[0][0]])

            with torch.no_grad():
                tempoutput = model(tempinputs)
            syn.append(F.cross_entropy(tempoutput, tlabel, reduction='none').item())
        # print(syn)
        tword = word_list[syn.index(max(syn))]
        return tword


#ä»å…¶å®ƒç±»ä¸”æ‹¼å†™ç›¸ä¼¼çš„å•è¯ä¸­æ‰¾å‡ºå¯¹ç½®ä¿¡åº¦å½±å“æœ€å¤§çš„è¯,ï¼ˆé»‘ç›’å¯ä»¥ç”¨ï¼Ÿå­˜ç–‘ï¼‰,textsæœªç»rawå¤„ç†
def most_largest_conf_sim(word_index, model, texts, vocab, word, word_list):
    similar_word_list = difflib.get_close_matches(word, word_list, 5, cutoff=0.6)
    similar_word_id_list = []
    for i in range(len(similar_word_list)):
        similar_word_id_list.append(utils.vocab[similar_word_list[i]])

    input_seq, f_label = utils.build_text(texts,vocab)

    # word_index = input_seq.index(utils.vocab[word])

    input_seq = torch.Tensor(input_seq).long().view(1, -1).unsqueeze(dim=0)
    if device:
        input_seq = input_seq.to(device)
    res = model(input_seq)

    tlabel = torch.max(res, 1)[1].view(-1)

    if len(similar_word_list):
        syn = []
        tempinputs = input_seq.clone()

        for similar_id in similar_word_id_list:

            tempinputs[0] = torch.LongTensor([similar_id if i==word_index else a for a in input_seq[0][0]])

            with torch.no_grad():
                tempoutput = model(tempinputs)
            syn.append(F.cross_entropy(tempoutput, tlabel, reduction='none').item())
        # print(syn)
        tword = similar_word_list[syn.index(max(syn))]
        return tword
    else:
        tword = find_most_similiar(word, word_list)
        if tword:
            return next(iter(tword))
        else:
            return visual(word, similar_0)






if __name__ == '__main__':

    torch.manual_seed(8)
    torch.cuda.manual_seed(8)

    embedding = 'embedding_banner.npz'
    # dataset = 'dataset-dt'
    dataset = 'dataset'

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    model = TextRNN_Att.Model(TextRNN_Att.Config(dataset, embedding))
    state = torch.load(path.p_rnnatt_modelpath, map_location=torch.device('cpu'))
    model = model.to(device)

    try:
        model.load_state_dict(state['state_dict'])
    except:
        model = torch.nn.DataParallel(model)
        model.load_state_dict(state['state_dict'])
        model = model.module
    print('Type input:')
    info = np.load(dataset + '/data/' + embedding, allow_pickle=True)
    # print(info.files)
    embeddings = info['embeddings'][()]
    vocab = utils.vocab
    # print(vocab['dvrh264'])
    # ind = most_largest_cos(embeddings, 'c1000z')
    # print(ind)
    # print(list(utils.vocab.keys())[list(utils.vocab.values()).index(ind)])
    # print(utils.vocab['c1000z'])
    # print(embeddings)
    # word_list = ['c1000z', 'vxworks5.4.2', '200', 'http','cl','1000']


    input = '5.4 thttpd dvrh264 25b last-modified bytes dvrh264 activex thttpd'
    input1 = input
    text = utils.preprocessing(input)
    texts = ' '.join(text)
    label = '66'
    hotwords = ['thttpd','5.4','dvrh264']
    def dictlist(filepath):
        # å°†é‡ç‚¹è¯è¡¨çš„txtè½¬æˆåˆ—è¡¨å‹å¼
        dicts = [line.strip() for line in open(filepath, 'r', encoding='utf-8').readlines()]
        return dicts
    word_list = dictlist('hotwords/grad_p_att_hotwords.txt')
    word_list.remove('thttpd')
    # word_list.remove('5.4')
    word_list.remove('dvrh264')
    # word_list = ['h267a', 'h2c','shop', 'sharp', 'happy', 'th', 'sh']
    # print(most_largest_cos_other(embeddings, 'c1000z',word_list))
    # print(generate_embed(embeddings, word_list))
    # print(embeddings[552])
    # print(embeddings[utils.vocab['dvrh264']])
    import time

    time_start = time.time()

    hhh = []
    for word in text:
        if word in hotwords:
            hhh.append(word)
    # print(hhh)
    for word in set(hhh):
        word_index_list = get_same_element_index(text, word)
        for word_index in word_index_list:
        # tword = most_largest_projection(model, texts,label, embeddings, word, word_list)
        #     tword = most_largest_cos_other_sim(embeddings,vocab, word, word_list)
        #     input = re.sub(word, tword, input,flags=re.IGNORECASE, count=1)
            tword = most_largest_conf_sim(word_index,model, texts, word, word_list)
            input = re.sub(word, tword, input, flags=re.IGNORECASE, count=1)

            tword1 = cos_proj_combine_sim(word_index,model, texts, embeddings,vocab, word, word_list)
            input1 = re.sub(word, tword1, input1,flags=re.IGNORECASE, count=1)
    print(input)
    print(input1)
    time_end = time.time()
    print('totally cost', time_end - time_start)
    # print(difflib.get_close_matches('hp', word_list, 5, cutoff=0.5))
